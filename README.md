# ğŸš€ Apache Spark RDD - Sales & Log Analysis Project

[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-4.0.1-orange?logo=apache-spark)](https://spark.apache.org/)
[![Java](https://img.shields.io/badge/Java-21-red?logo=java)](https://www.oracle.com/java/)
[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.6-yellow?logo=apache-hadoop)](https://hadoop.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue?logo=docker)](https://www.docker.com/)

A comprehensive Big Data project implementing **two real-world data analysis exercises** using Apache Spark RDD API in Java. The project demonstrates distributed data processing with both local and cluster execution modes.

---

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Technologies Stack](#technologies-stack)
- [Architecture](#architecture)
- [Exercise 1: Sales Data Analysis](#exercise-1-sales-data-analysis)
  - [Local Execution](#exercise-1-local-execution)
  - [Docker Cluster Execution](#exercise-1-docker-cluster-execution)
- [Exercise 2: Web Server Log Analysis](#exercise-2-web-server-log-analysis)
  - [Local Execution](#exercise-2-local-execution)
  - [Docker Cluster Execution](#exercise-2-docker-cluster-execution)
- [Installation & Setup](#installation--setup)
- [Results & Performance](#results--performance)
- [Key Learnings](#key-learnings)
- [Contributors](#contributors)

---

## ğŸ¯ Project Overview

This project implements **two comprehensive data analysis exercises** using Apache Spark's RDD (Resilient Distributed Dataset) API:

### Exercise 1: Sales Data Analysis
Analyze sales data from retail transactions to calculate:
- **App1**: Total sales amount by city
- **App2**: Total sales amount by city and year

### Exercise 2: Web Server Log Analysis
Parse and analyze Apache web server logs to extract:
- Basic statistics (total requests, errors, error percentage)
- Top 5 IP addresses with most requests
- Top 5 most requested resources
- HTTP status code distribution

**Both exercises are executed in two modes:**
- âœ… **Local Mode**: For development and testing
- âœ… **Distributed Mode**: On Docker cluster with Spark Master + 2 Workers

---

## ğŸ’» Technologies Stack

| Technology | Version | Purpose |
|------------|---------|---------|
| **Apache Spark** | 4.0.1 | Distributed data processing engine |
| **Apache Hadoop** | 3.3.6 | HDFS distributed storage + YARN resource manager |
| **Java** | 21 | Application development language |
| **Maven** | 3.9 | Build automation and dependency management |
| **Docker** | 20.10+ | Containerization and cluster orchestration |
| **Docker Compose** | 2.0+ | Multi-container application management |

---

## ğŸ—ï¸ Architecture

### Cluster Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HADOOP + SPARK CLUSTER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   NameNode   â”‚  â”‚  DataNode    â”‚  â”‚ Resource Mgr â”‚      â”‚
â”‚  â”‚   (HDFS)     â”‚  â”‚   (HDFS)     â”‚  â”‚    (YARN)    â”‚      â”‚
â”‚  â”‚ Port: 9870   â”‚  â”‚              â”‚  â”‚  Port: 8088  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Spark Master â”‚  â”‚Spark Worker 1â”‚  â”‚Spark Worker 2â”‚      â”‚
â”‚  â”‚ Port: 8085   â”‚  â”‚  Port: 8081  â”‚  â”‚  Port: 8083  â”‚      â”‚
â”‚  â”‚  2 Cores     â”‚  â”‚  1G RAM      â”‚  â”‚  1G RAM      â”‚      â”‚
â”‚  â”‚  2GB RAM     â”‚  â”‚  1 Core      â”‚  â”‚  1 Core      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
<img width="1200" height="700" alt="image" src="https://github.com/user-attachments/assets/8c42ca2c-46f7-4adf-884d-290afe631781" />

### Data Flow

```
Local Files â†’ HDFS â†’ Spark RDD â†’ Transformations â†’ Actions â†’ Results
```

---

## ğŸ“Š Exercise 1: Sales Data Analysis

### Problem Statement

Analyze sales transactions to extract business insights from sales data.

**Data Format:**
```
date city product price
```

**Sample Data (ventes.txt):**
```
2023 Casablanca Chaise 850.0
2023 Rabat Table 1200.0
2024 Casablanca Canape 3500.0
2024 Rabat Etagere 950.0
2024 Marrakech Chaise 900.0
```

### Question 1: Total Sales by City

**Implementation: App1.java**

<img width="1261" height="920" alt="Screenshot 2025-11-15 203856" src="https://github.com/user-attachments/assets/2e3f2760-c0d3-47a5-8f50-ef91d8d96ec5" />


**Implementation: App2.java**

<img width="1499" height="882" alt="Screenshot 2025-11-15 203952" src="https://github.com/user-attachments/assets/a1ff93ba-1fce-44cd-ace3-e5afbca4c439" />



### Exercise 1: Local Execution

**Commands:**
```bash
# Compile the project
mvn clean package

# Run App1 locally
java -cp target/tp3-spark-with-rdd-1.0-SNAPSHOT.jar ma.bigdata.exercice1.App1

# Run App2 locally  
java -cp target/tp3-spark-with-rdd-1.0-SNAPSHOT.jar ma.bigdata.exercice1.App2
```

**Results - App1 (Sales by City):**

<img width="1688" height="745" alt="Screenshot 2025-11-15 201207" src="https://github.com/user-attachments/assets/e6977f81-f5e4-4ba0-a52f-cc8b1b80a8b3" />
```
========== APP1 RESULTS ==========
Marrakech : 3000.00 DH
Rabat : 2150.00 DH
Fes : 450.00 DH
Agadir : 1100.00 DH
Casablanca : 4830.00 DH
Tanger : 1950.00 DH
==================================
```

**Results - App2 (Sales by City and Year):**

<img width="1736" height="741" alt="Screenshot 2025-11-15 201146" src="https://github.com/user-attachments/assets/a057ee08-8bd5-4593-9a05-6e7c516a95a4" />
```
========== APP2 RESULTS ==========
2023 | Agadir | 1100.00 DH
2023 | Casablanca | 850.00 DH
2023 | Fes | 450.00 DH
2023 | Marrakech | 2100.00 DH
2023 | Rabat | 1200.00 DH
2023 | Tanger | 1950.00 DH
2024 | Casablanca | 3980.00 DH
2024 | Marrakech | 900.00 DH
2024 | Rabat | 950.00 DH
==================================
```

---

### Exercise 1: Docker Cluster Execution

**Step 1: Start the Cluster**
```bash
docker-compose up -d
```

<img width="950" height="818" alt="Screenshot 2025-11-15 212237" src="https://github.com/user-attachments/assets/ad081fc6-c662-4c4b-8886-fb06c0bfd27a" />

<img width="749" height="932" alt="Screenshot 2025-11-15 212325" src="https://github.com/user-attachments/assets/08f63f87-eb68-4dbe-bc41-3dbc37a236cc" />

**Step 2: Upload Data to HDFS**
```bash
# Create directory in HDFS
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -mkdir -p /data

# Upload file
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -put /input-data/ventes.txt /data/

# Verify
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -ls /data/
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -cat /data/ventes.txt
```

<img width="1717" height="441" alt="Screenshot 2025-11-15 215033" src="https://github.com/user-attachments/assets/cd32548d-06bd-4217-9395-c47c78985ba2" />

<img width="1792" height="248" alt="Screenshot 2025-11-15 221154" src="https://github.com/user-attachments/assets/41235ee9-d26a-47c1-9145-95b10c4d41d1" />

<img width="1900" height="990" alt="Screenshot 2025-11-16 132240" src="https://github.com/user-attachments/assets/eaf35b57-178c-4408-8351-cf54a476fc32" />


**Step 3: Submit Applications to Spark Cluster**
```bash
# Submit App1
docker exec spark-master /opt/spark/bin/spark-submit \
  --class ma.bigdata.exercice1.App1 \
  --master spark://spark-master:7077 \
  --executor-memory 1g \
  --total-executor-cores 2 \
  /opt/spark-apps/tp3-spark-with-rdd-1.0-SNAPSHOT.jar

# Submit App2
docker exec spark-master /opt/spark/bin/spark-submit \
  --class ma.bigdata.exercice1.App2 \
  --master spark://spark-master:7077 \
  --executor-memory 1g \
  --total-executor-cores 2 \
  /opt/spark-apps/tp3-spark-with-rdd-1.0-SNAPSHOT.jar
```
App 1:

<img width="1785" height="793" alt="Screenshot 2025-11-16 132639" src="https://github.com/user-attachments/assets/10f30c1e-b9af-49fa-9c1c-6c77a21d08f3" />
<img width="1780" height="762" alt="Screenshot 2025-11-16 132655" src="https://github.com/user-attachments/assets/7856c510-b9a5-449b-ae7b-fa4bd06d316f" />
<img width="1779" height="727" alt="Screenshot 2025-11-16 132708" src="https://github.com/user-attachments/assets/e177200c-0ff7-4ee2-a44b-859c7857550b" />

App 2 :
<img width="1785" height="581" alt="Screenshot 2025-11-16 134846" src="https://github.com/user-attachments/assets/173e082e-e279-40af-bf7f-94895103df63" />
<img width="1798" height="735" alt="Screenshot 2025-11-16 134916" src="https://github.com/user-attachments/assets/f843f983-d30e-4b30-9a4d-ff7dff1ae34f" />



**Cluster Monitoring:**

<img width="1918" height="1039" alt="Screenshot 2025-11-16 170248" src="https://github.com/user-attachments/assets/49c47279-9445-42fa-876b-3857bd22a57c" />

*Spark Master UI showing 2 alive workers and completed applications*


<img width="1919" height="1042" alt="Screenshot 2025-11-16 170305" src="https://github.com/user-attachments/assets/c3b4f35e-a5fd-4911-bfe3-2090473f9ffa" />
<img width="1919" height="1070" alt="Screenshot 2025-11-16 170324" src="https://github.com/user-attachments/assets/1821d4ec-7027-4586-b87a-91b48d73ff90" />

*Worker details showing finished executors for both applications*

**Results:**
Both applications produced identical results when running on the cluster, demonstrating:
- âœ… Distributed processing across 2 workers
- âœ… Data locality with HDFS
- âœ… Fault tolerance
- âœ… Resource management by YARN


---

## ğŸ” Exercise 2: Web Server Log Analysis

### Problem Statement

Analyze Apache web server access logs to extract valuable insights about server usage patterns, errors, and user behavior.

**Log Format:**
```
IP - user [date:time +zone] "METHOD resource PROTOCOL" code size "referer" "user-agent"
```

**Sample Data (access.log):**
```
127.0.0.1 - - [10/Oct/2025:09:15:32 +0000] "GET /index.html HTTP/1.1" 200 1024 "http://example.com" "Mozilla/5.0"
192.168.1.10 - john [10/Oct/2025:09:17:12 +0000] "POST /login HTTP/1.1" 302 512 "-" "curl/7.68.0"
203.0.113.5 - - [10/Oct/2025:09:19:01 +0000] "GET /docs/report.pdf HTTP/1.1" 404 64 "-" "Mozilla/5.0"
```

### Implementation: LogAnalysis.java

**Key Features:**
- âœ… Regex pattern matching for log parsing
- âœ… Extraction of 6 fields: IP, DateTime, Method, Resource, HTTP Code, Size
- âœ… Error handling for malformed entries
- âœ… Multiple aggregation operations

```java
// Log parsing with Regex
String pattern = "^(\\S+) \\S+ \\S+ \\[([^\\]]+)\\] \"(\\S+) (\\S+) \\S+\" (\\d+) (\\d+).*";
Pattern r = Pattern.compile(pattern);
Matcher m = r.matcher(line);

if (m.find()) {
    String ip = m.group(1);           // IP address
    String dateTime = m.group(2);     // Date/time
    String method = m.group(3);       // HTTP method
    String resource = m.group(4);     // Requested resource
    int httpCode = Integer.parseInt(m.group(5));  // HTTP status code
    int size = Integer.parseInt(m.group(6));      // Response size
    
    return new LogEntry(ip, dateTime, method, resource, httpCode, size);
}
```

---

### Exercise 2: Local Execution

**Command:**
```bash
java -cp target/tp3-spark-with-rdd-1.0-SNAPSHOT.jar ma.bigdata.exercice2.LogAnalysis
```

**Complete Analysis Report:**
<img width="1792" height="688" alt="Screenshot 2025-11-16 162608" src="https://github.com/user-attachments/assets/8766bf7d-23a9-435b-b147-b874d5032248" />
<img width="1792" height="706" alt="Screenshot 2025-11-16 162633" src="https://github.com/user-attachments/assets/ac3157d0-6db0-4c88-99a6-0786a06dfc79" />
<img width="1791" height="689" alt="Screenshot 2025-11-16 162652" src="https://github.com/user-attachments/assets/2dd3ea45-adeb-4d49-902f-6b1065c7a090" />
<img width="1848" height="689" alt="Screenshot 2025-11-16 162823" src="https://github.com/user-attachments/assets/84f2765f-2bdf-43a5-9cdc-bde5ca7e74a2" />


**Analysis Insights:**
- âœ… **68% success rate** indicates healthy server performance
- âš ï¸ **24% error rate** requires attention:
  - 12% are 404 errors (broken links or missing resources)
  - 8% are 500 errors (server-side issues need investigation)
- ğŸ“Š **Traffic distribution** is even across 5 IP addresses
- ğŸ¯ **/dashboard** is the most popular endpoint

---

### Exercise 2: Docker Cluster Execution

<img width="1793" height="898" alt="Screenshot 2025-11-16 201755" src="https://github.com/user-attachments/assets/9df350f2-a2dd-4185-868e-2f013e7c5b71" />

**Step 1: Upload Logs to HDFS**

```bash
# Create logs directory
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -mkdir -p /logs

# Upload access.log
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -put /input-data/access.log /logs/

# Verify upload
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -ls /logs/
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -cat /logs/access.log | head -5
```

**Step 2: Submit to Spark Cluster**
```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --class ma.bigdata.exercice2.LogAnalysis \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --executor-memory 1g \
  --total-executor-cores 2 \
  /opt/spark-apps/tp3-spark-with-rdd-1.0-SNAPSHOT.jar
```
<img width="1913" height="1049" alt="Screenshot 2025-11-16 210133" src="https://github.com/user-attachments/assets/a4ba0a85-e808-41e0-b072-edfdd4936104" />
<img width="1919" height="991" alt="Screenshot 2025-11-16 210211" src="https://github.com/user-attachments/assets/ab8a679a-6e21-423f-bc64-1b5e6cb5ca01" />
<img width="1919" height="999" alt="Screenshot 2025-11-16 210111" src="https://github.com/user-attachments/assets/54fede7e-70fc-4e99-aed9-a76caea41e5d" />


**Cluster Monitoring:**

The application successfully executed on the distributed cluster with:
- âœ… Data read from HDFS (`hdfs://namenode:8020/logs/access.log`)
- âœ… Processing distributed across 2 Spark workers
- âœ… Identical results to local execution
- âœ… Completed in ~6 seconds with distributed processing

**Results:**
All 6 analysis questions were answered successfully with identical results to local execution, proving:
- Consistency across execution modes
- Correctness of distributed processing
- Proper HDFS integration
- Efficient resource utilization

---

## ğŸ“¦ Installation & Setup

### Prerequisites

```bash
# Required software
- Docker Desktop 20.10+
- Docker Compose 2.0+
- Java JDK 21
- Maven 3.9 (optional, can use Docker)
```

### Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/malakzaidi/Tp3-spark-with-rdd.git
cd Tp3-spark-with-rdd

# 2. Create data files
mkdir -p data
# Add ventes.txt and access.log to data/ folder

# 3. Compile the project
mvn clean package
# OR using Docker
docker run --rm -v ${PWD}:/app -w /app maven:3.9-eclipse-temurin-21 mvn clean package

# 4. Run locally
java -cp target/tp3-spark-with-rdd-1.0-SNAPSHOT.jar ma.bigdata.exercice1.App1
java -cp target/tp3-spark-with-rdd-1.0-SNAPSHOT.jar ma.bigdata.exercice2.LogAnalysis

# 5. Deploy to cluster
docker-compose up -d
# Upload data to HDFS and submit jobs (see commands above)
```

### Project Structure

```
tp3-spark-with-rdd/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â””â”€â”€ java/
â”‚           â””â”€â”€ ma/bigdata/
â”‚               â”œâ”€â”€ exercice1/
â”‚               â”‚   â”œâ”€â”€ App1.java              # Sales by city
â”‚               â”‚   â””â”€â”€ App2.java              # Sales by city & year
â”‚               â””â”€â”€ exercice2/
â”‚                   â”œâ”€â”€ LogAnalysis.java       # Log analysis
â”‚                   â””â”€â”€ data/
â”‚                       â””â”€â”€ access.log         # Sample logs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ventes.txt                            # Sales data
â”‚   â””â”€â”€ access.log                            # Server logs
â”œâ”€â”€ target/
â”‚   â””â”€â”€ tp3-spark-with-rdd-1.0-SNAPSHOT.jar  # Compiled JAR
â”œâ”€â”€ docker-compose.yml                        # Cluster configuration
â”œâ”€â”€ pom.xml                                   # Maven dependencies
â””â”€â”€ README.md                                 # This file
```

---

## ğŸ“ˆ Results & Performance

### Performance Comparison

| Metric | Local Execution | Cluster Execution |
|--------|----------------|-------------------|
| **Processing Time (App1)** | ~2 seconds | ~6 seconds |
| **Processing Time (App2)** | ~2 seconds | ~6 seconds |
| **Processing Time (LogAnalysis)** | ~3 seconds | ~8 seconds |
| **Resource Utilization** | Single JVM | 2 workers (distributed) |
| **Data Location** | Local filesystem | HDFS (replicated) |
| **Scalability** | Limited to 1 machine | Horizontal scaling |
| **Fault Tolerance** | None | Automatic recovery |

**Note:** Cluster execution shows higher latency for small datasets due to network overhead and task distribution. For larger datasets (GB/TB), cluster execution would significantly outperform local mode.

### Key Insights

#### Exercise 1 - Sales Analysis
- **Casablanca** leads with 4,830 DH in total sales
- **2024** shows higher sales than 2023 in most cities
- Clear trend of business growth year-over-year

#### Exercise 2 - Log Analysis
- **68% success rate** - healthy server performance
- **24% error rate** - requires investigation:
  - Focus on fixing 404 errors (broken links)
  - Debug 500 errors (server issues)
- **/dashboard** is the most accessed resource
- Traffic evenly distributed across users

---

## ğŸ“ Key Learnings

### Technical Skills Acquired

âœ… **Apache Spark RDD API**
- Transformations: `map`, `flatMap`, `filter`, `flatMapToPair`
- Actions: `collect`, `count`, `reduce`, `take`
- Pair RDD operations: `reduceByKey`, `sortByKey`, `mapToPair`

âœ… **Distributed Computing Concepts**
- Data partitioning and distribution
- Task scheduling and execution
- Fault tolerance mechanisms
- Resource management with YARN

âœ… **HDFS Integration**
- Data upload and retrieval
- Distributed storage benefits
- Data locality optimization

âœ… **Real-World Data Processing**
- CSV/Text file parsing
- Log file analysis with Regex
- Aggregation and statistical analysis
- Error handling and data validation

âœ… **Docker & Containerization**
- Multi-container orchestration
- Volume mounting
- Network configuration
- Service dependencies

### Best Practices Implemented

- ğŸ—ï¸ **Modular Code Structure** - Separate applications for different analyses
- ğŸ“ **Comprehensive Logging** - Clear output formatting
- ğŸ”’ **Error Handling** - Graceful handling of malformed data
- ğŸ“Š **Performance Optimization** - Use of `cache()` for reused RDDs
- ğŸ§ª **Testing Strategy** - Local testing before cluster deployment
- ğŸ“š **Documentation** - Well-commented code and clear README

---

## ğŸ”— Useful Commands

### Docker Management
```bash
# Start cluster
docker-compose up -d

# Stop cluster
docker-compose down

# View logs
docker-compose logs -f spark-master

# Restart services
docker-compose restart

# Check status
docker-compose ps
```

### HDFS Commands
```bash
# List files
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -ls /data/

# Create directory
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -mkdir -p /output

# Upload file
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -put local.txt /data/

# Download file
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -get /data/file.txt ./

# View content
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -cat /data/file.txt

# Delete file
docker exec tp3-spark-with-rdd-namenode-1 hdfs dfs -rm /data/file.txt
```

### Web UIs
```bash
# Spark Master: http://localhost:8085
# Spark Worker 1: http://localhost:8081
# Spark Worker 2: http://localhost:8083
# HDFS NameNode: http://localhost:9870
# YARN ResourceManager: http://localhost:8088
# Spark Application UI: http://localhost:4040 (when app is running)
```

---

## ğŸ¤ Contributors

- **Malak Zaidi** - [@malakzaidi](https://github.com/malakzaidi)
  - Project Development
  - RDD Implementation
  - Cluster Configuration
  - Documentation

---

## ğŸ“„ License

This project is developed for educational purposes as part of a Big Data course.

---

## ğŸ™ Acknowledgments

- **Pr. Abdelmajid BOUSSELHAM** - Course Instructor
- **Apache Spark Community** - For excellent documentation
- **Hadoop Ecosystem** - For distributed computing infrastructure

---

## ğŸ“ Contact

For questions or suggestions, please open an issue on this repository.

**Repository**: [https://github.com/malakzaidi/Tp3-spark-with-rdd](https://github.com/malakzaidi/Tp3-spark-with-rdd)

---
***If this project helped you please star it ^^^***

**Last Updated**: November 2025  
**Status**: âœ… Complete - All exercises implemented and tested  
